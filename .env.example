# --- Configuración de Qdrant ---
# Estas variables son para la conexión a la base de datos vectorial Qdrant.
# Si usas Docker Compose como se provee, QDRANT_HOST debe ser el nombre del servicio.
QDRANT_HOST=qdrant
QDRANT_PORT=6333

# --- Configuración del Vectorizador y RAG ---
# Nombre de la colección en Qdrant donde se almacenarán los vectores.
# IMPORTANTE: Debe ser el mismo en la ingesta y en la API.
COLLECTION_NAME="perfil_profesional"

# --- Credenciales de APIs de Modelos de Lenguaje (LLM) ---
# Descomenta y rellena las claves para el servicio que vayas a utilizar en rag_api.py.

# Para Google Gemini (tanto para embeddings como para el LLM)
# GOOGLE_API_KEY="tu-api-key-de-google"

# Para Groq (muy rápido para inferencia)
GROQ_API_KEY="tu-api-key-de-groq"

# Para OpenAI
# OPENAI_API_KEY="tu-api-key-de-openai"

# --- Configuración del Prompt del Sistema ---
# Este es el prompt que instruye al LLM sobre cómo debe comportarse.
# Puedes modificarlo para ajustar el tono y la personalidad del asistente.
SYSTEM_PROMPT="""Eres un asistente de IA experto y amigable. Tu tarea es responder preguntas sobre el perfil profesional de [Tu Nombre], basándote únicamente en el contexto proporcionado.

Contexto: {context}

Pregunta: {question}

Instrucciones:
1.  Sé conciso y ve al grano.
2.  Si la respuesta no está en el contexto, di amablemente: 'No tengo información sobre eso en mis documentos'.
3.  No inventes información.
4.  Responde en español."""

# --- Configuración del Modelo de Lenguaje ---
# Define el nombre del modelo que se usará para la generación de respuestas.
# Asegúrate de que el modelo sea compatible con el proveedor de API que estás utilizando.
# Ejemplos:
# Para Groq: "llama3-70b-8192", "gemma-7b-it"
# Para Google: "gemini-1.5-flash", "gemini-pro"
# Para OpenAI: "gpt-4o", "gpt-3.5-turbo"
MODEL_NAME="llama3-70b-8192"
